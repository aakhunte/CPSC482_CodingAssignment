{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of Abnormalities in Single-Lead ECGs in the PTB-XL ECG Dataset\n",
    "#### Akshay Khunte\n",
    "#### CPCS 482: Current Topics in Applied Machine Learning\n",
    "#### March 3, 2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries\n",
    "The conda environment required to import the modules required for running this model is saved to the environment.yml file in this GitHub repo, and can be built using the command line with the command \"conda env create -f environment.yml\", which will build the environment \"tf\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 21:52:35.942293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-03 21:52:36.966064: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-03 21:52:36.966128: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-03 21:52:36.966135: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras  # tf.keras\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Activation, Add, AveragePooling1D, AveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Concatenate, Conv1D, Conv2D, Dense, Flatten\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import Reshape, Input, MaxPooling2D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "To download data, use either \"wget -r -N -c -np https://physionet.org/files/ptb-xl/1.0.3/\" in terminal or download ZIP file from https://physionet.org/content/ptb-xl/1.0.3/\n",
    "\n",
    "Much of the code in this section was sourced from the PTB-XL dataset release above, with a few modifications (denoted by comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import ast\n",
    "\n",
    "# this function implemented by me; isolates the first lead of the signal and applies a median filtering approach to remove baseline wander\n",
    "def preprocess(signal, sampling_rate):\n",
    "    signal = signal.T[0] # isolates first lead\n",
    "    signal = signal - median_filter(signal, size = (sampling_rate,))\n",
    "\n",
    "    return signal\n",
    "\n",
    "# this function is as implemented by PTB-XL publishers, but modified as denoted to use my custom preprocess function\n",
    "def load_raw_data(df, sampling_rate, path):\n",
    "    if sampling_rate == 100:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_lr]\n",
    "    else:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_hr]\n",
    "    data = np.array([preprocess(signal, sampling_rate) for signal, meta in data]) # this line is modified with the \"preprocess(signal, sampling_rate)\" instead of \"signal\" as in the inital code\n",
    "\n",
    "    return data\n",
    "\n",
    "# this path should be set to the directory at which this dataset is downloaded.\n",
    "# If this is downloaded from the internet,the path will follow the format of the commented out path;\n",
    "# if downloaded using wget, use the path that is not commented out.\n",
    "# path = '~/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/'\n",
    "path = '/home/ak2532/physionet.org/files/ptb-xl/1.0.3/'\n",
    "sampling_rate=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and convert annotation data\n",
    "Y = pd.read_csv(path+'ptbxl_database.csv', index_col='ecg_id')\n",
    "Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw signal data\n",
    "X = load_raw_data(Y, sampling_rate, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom code; allows for usage of 2D CNN instead of 1D CNN\n",
    "X = np.expand_dims(X, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function written by me, to identify ECGs as abnormal (\"TRUE\", or Positive) or normal (\"FALSE\", or Control) based on associated SCP codes for training the model\n",
    "def abnormal_ecg(row):\n",
    "    if 'NORM' in row['scp_codes'].keys():\n",
    "        return False # normal ECG\n",
    "    else:\n",
    "        return True # abnormal ECG\n",
    "    \n",
    "Y['abnormal_ECG'] = Y.apply(lambda row: abnormal_ecg(row), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and test sets (random_state used for consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y['abnormal_ECG'], test_size = .1, random_state = 12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model\n",
    "\n",
    "I developed this model from scratch for the detection of both easily diaganosable and hidden cardiovascular diseases from ECGs. More information about the model can be found at my preprint here: https://www.medrxiv.org/content/10.1101/2022.12.03.22283065v1.\n",
    "\n",
    "The model is a small (30k param) CNN which incorporates dropout as a strategy to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are two helper functions used my the \"cnn\" function\n",
    "\n",
    "def conv2d_bn_mp(x,\n",
    "              filters,\n",
    "              kernel_size,\n",
    "              pool_size,\n",
    "              strides=1,\n",
    "              padding='same',\n",
    "              activation='relu',\n",
    "              use_bias=False,\n",
    "              name=None):\n",
    "    \"\"\"Utility function to apply conv + BN.\n",
    "    # Returns\n",
    "        Output tensor after applying `Conv2D` and `BatchNormalization`.\n",
    "    \"\"\"\n",
    "    x = Conv2D(filters,\n",
    "               kernel_size,\n",
    "               strides=strides,\n",
    "               padding=padding,\n",
    "               use_bias=use_bias,\n",
    "               name=name)(x)\n",
    "    # if not use_bias:\n",
    "    #     bn_axis = 3\n",
    "    #     bn_name = _generate_layer_name('BatchNorm', prefix=name)\n",
    "    #     x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n",
    "    # if activation is not None:\n",
    "    #     ac_name = _generate_layer_name('Activation', prefix=name)\n",
    "    #     x = Activation(activation, name=ac_name)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    x = MaxPooling2D(pool_size = pool_size, padding = 'same')(x)\n",
    "    return x\n",
    "\n",
    "def fc_layer(x, units, dropout_rate = 0.2, activation = 'relu'):\n",
    "    x = Dense(units)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(input_shape, dropout_rate):\n",
    "    # input shape: (None, Signal Length, Num Leads, 1)\n",
    "    # dropout_rate: float from 0->1\n",
    "    # include_top : keep prediction output layer or not\n",
    "\n",
    "    inputs = Input(batch_shape = input_shape, name = 'input')\n",
    "    x = conv2d_bn_mp(inputs, filters = 16, kernel_size = (7,1), pool_size = (2,1))\n",
    "    x = conv2d_bn_mp(x, filters = 16, kernel_size = (7,1), pool_size = (2,1))\n",
    "    x = conv2d_bn_mp(x, filters = 16, kernel_size = (5,1), pool_size = (2,1))\n",
    "    x = conv2d_bn_mp(x, filters = 32, kernel_size = (5,1), pool_size = (4,1))\n",
    "    x = conv2d_bn_mp(x, filters = 32, kernel_size = (3,1), pool_size = (2,1))\n",
    "    x = conv2d_bn_mp(x, filters = 64, kernel_size = (3,1), pool_size = (2,1))\n",
    "    x = conv2d_bn_mp(x, filters = 64, kernel_size = (3,1), pool_size = (4,1))\n",
    "\n",
    "    x = fc_layer(x, 64, dropout_rate = dropout_rate)\n",
    "    x = fc_layer(x, 32, dropout_rate = dropout_rate)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(1, activation = 'sigmoid', name = 'output')(x)\n",
    "    model = Model(inputs, outputs, name = 'cnn')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Loss function which generates weights based on the specific distribution of positives/control in the dataset used for training. This was also developed on my own\n",
    "class CustomLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, class_names, df):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weights = self.calc_weights(class_names, df)\n",
    "        # self.weights = np.array([[1, 0.1]])\n",
    "\n",
    "    def get_effective_weights(self, samples_per_class, n_classes = 2, beta = .999995):\n",
    "        effective_num = 1.0-np.power(beta, samples_per_class)\n",
    "        weights = (1.0 - beta) / np.array(effective_num)\n",
    "        weights = weights / np.sum(weights) * n_classes\n",
    "        return(weights)\n",
    "\n",
    "    def calc_weights(self, class_names, df):    \n",
    "        # class_names: labels for which weights should be calculated in format ['label1', 'label2']\n",
    "        # df: pandas dataframe with columns in 'class_names' for which weights should be calculated\n",
    "\n",
    "        weights = np.empty([len(class_names),2])\n",
    "        for c in range(len(class_names)):\n",
    "            one = np.count_nonzero(df[class_names[c]]==1)\n",
    "            zero = np.count_nonzero(df[class_names[c]]==0)\n",
    "            weights[c] = self.get_effective_weights([one,zero], beta = .999995)\n",
    "            #weights[c][0] = labels.shape[0]/(2*np.count_nonzero(labels[class_names[c]]==1))\n",
    "            #weights[c][1] = labels.shape[0]/(2*np.count_nonzero(labels[class_names[c]]==0))\n",
    "        \n",
    "        return weights\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        weights = self.weights\n",
    "        weights = np.array(weights)\n",
    "        return K.mean((weights[:,1]**(1-y_true))*(weights[:,0]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "batch_size = 64\n",
    "dropout = 0.5\n",
    "epochs = 200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 21:57:22.455070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2006] Ignoring visible gpu device (device: 3, name: Quadro P600, pci bus id: 0000:05:00.0, compute capability: 6.1) with core count: 3. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\n",
      "2023-03-03 21:57:22.461023: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-03 21:57:24.479442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7759 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1\n",
      "2023-03-03 21:57:24.501399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7751 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\n",
      "2023-03-03 21:57:24.519521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 7751 MB memory:  -> device: 2, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "model = cnn((None, 1000, 1, 1), dropout)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer = opt, loss=CustomLoss(['abnormal_ECG'], Y), metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC(curve='ROC'), tf.keras.metrics.AUC(curve='PR')])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish Prepping Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires a model_checkpoints folder, where these model checkpoints are saved; added to gitignore\n",
    "save_path = f'/home/ak2532/CPSC482_CodingAssignment/model_checkpoints/LR{learning_rate}_dropout{dropout}/trained_model'\n",
    "saved_model_file = save_path + '_{epoch:02d}'\n",
    "\n",
    "# save each model epoch's weights\n",
    "checkpoint = ModelCheckpoint(saved_model_file, monitor='val_loss', save_format = 'tf', save_weights_only = True, save_best_only=False, verbose=1)\n",
    "\n",
    "# ensure that model stops training after overfitting for three epochs\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# log results from each epoch to a CSV\n",
    "log_path = f'/home/ak2532/CPSC482_CodingAssignment/model_checkpoints/LR{learning_rate}_dropout{dropout}/'\n",
    "csv_logger = CSVLogger(log_path + f'training.csv', append = True)\n",
    "\n",
    "callbacks = [checkpoint, early_stop, csv_logger]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_train,\n",
    "    y=y_train,\n",
    "    validation_split = 0.1,\n",
    "    verbose = 1,\n",
    "    epochs = epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks = callbacks,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model on Test Set\n",
    "Best epoch chosen by selecting epoch with highest val AUROC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model in Held-Out Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 7ms/step - loss: 0.3959 - binary_accuracy: 0.8064 - auc: 0.8912 - auc_1: 0.9269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3959450423717499,\n",
       " 0.8064219951629639,\n",
       " 0.8911879062652588,\n",
       " 0.9268943071365356]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code was to evaluate the last epoch of models trained with different LRs against the test set.\n",
    "# The last epoch (epoch 48) of the model trained at a LR of 2e-5 performed best\n",
    "# lr = 2e-5\n",
    "epoch = 48\n",
    "# model.load_weights(f'/home/ak2532/CPSC482_CodingAssignment/model_checkpoints/LR{lr}_dropout0.5/trained_model_{epoch:02d}')\n",
    "\n",
    "# these weights have been moved to this GitHub Repo, so they can be loaded for evaluation\n",
    "# a copy of the associated training.csv file has also been added to the repo for training/val auroc metrics during training.\n",
    "model.load_weights(f'/home/ak2532/CPSC482_CodingAssignment/trained_model_{epoch:02d}')\n",
    "model.evaluate(x=X_test, y = y_test, batch_size = batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REPORT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial intelligence (AI) has been shown to diagnose many different diseases from electrocardiograms (ECGs), including diagnoses that have traditionally relied on comprehensive echocardiography or other cardiac imaging (Attia et al., 2019; Galasko et al., 2006). Even though AI-ECG is a promising screening tool for many cardiovascular conditions, most algorithms have been designed in clinically obtained 12-lead ECGs. Advances in wearable and handheld technologies now enable the point-of-care acquisition of single-lead ECG signals, paving the path for efficient and scalable AI screening tools for use with these technologies (Duarte et al., 2019; Kamga et al., 2022). This improved accessibility could enable broader AI-based screening for cardiovascular condition, and has already began implementation on wearable devices like the Apple Watch (Seshadri et al., 2020).  \n",
    "\n",
    "This project sought to develop an algorithm using the PTB-XL dataset for the detection of abnormal single-lead ECGs, which could be deployed using wearable ECG sensors so that patients with rhythmic or conductive abnormalities could be flagged and directed to seek appropriate medical care."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHODS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Source"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw voltage data for lead I was isolated from 12-lead ECGs included in PTB-XL, a large, publicly available dataset of ECGs published in 2020 (Wagner et al., 2020). Lead I was chosen as it represents the standard lead obtained from wearable devices (Duarte et al., 2019). PTB-XL includes each ECG in a 100 Hz and 500 Hz sampling frequencies. The 100 Hz sampling frequency recordings were selected for model development to allow for greater parity with wearable device ECG recordings, which have variable sampling rates and may require down sampling to reduce noise. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard preprocessing strategy was used to isolate signal from lead I of 12-lead ECGs, which included median pass filtering. The Lead I signal was isolated from each ECG, and a one second median filter was calculated for and subtracted from each single-lead ECG to remove baseline drift. The PTB-XL ECGs were already scaled to millivolts, which remained unchanged."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outcome Label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ECGs were marked as Normal (controls), or Abnormal (positives) by searching through the SCP codes associated with each ECG in the PTB-XL dataset. If an ECG had a code “NORM”, representing a normal rhythm, in the set of SCP codes affiliated with that ECG, that ECG was labeled with a “False” label, marking it as a control. All other ECGs were labeled with a “true” label, marking it as a positive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ECGs were randomly subset into a training/validation and a held-out test set (90%, 10%). A random seed was used to allow for split replication. A second 90%-10% split of the training/validation set was used during training to create separate training and validation sets. A CNN architecture previously reported to diagnose left ventricular systolic dysfunction with significant accuracy was used after slight modifications to accept 100 Hz ECGs (Khunte et al., 2022). This architecture consisted of a (1000, 1, 1) input layer, corresponding to a 10-second, 100 Hz, Lead I ECG, followed by seven two-dimensional convolutional layers, each of which were followed by a batch normalization layer (Ioffe & Szegedy, 07--09 Jul 2015), ReLU activation layer, and a two-dimensional max-pooling layer. The output of the seventh convolutional layer was then taken as input into a fully connected network consisting of two dense layers, each of which were followed by a batch normalization layer, ReLU activation layer, and a dropout layer with a dropout rate of 0.5 (Hinton et al., 2012). The output layer was a dense layer with one class and a sigmoid activation function. Model weights were calculated for the loss function such that learning was not affected by the lower frequency of abnormal ECGs compared to normal ECGs using the effective number of samples class re-weighting scheme. Cui et al., n.d.).  \n",
    "\n",
    "The model was trained on the Keras framework in TensorFlow 2.9.1 and Python 3.9 using the Adam optimizer. The final model was trained at a learning rate of 0.00002 until performance on the validation set did not improve for three consecutive epochs. The epoch with the highest performance on the validation set was selected for evaluation on the test set. Other learning rates, including 0.001, 0.0001, and 0.00001 were also evaluated, but did not meet the same performance of the final model in the validation subset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained to a maximum training and validation AUROC of 0.867 and 0.885, respectively. The models had a maximum training and validation AURPC of 0.912 and 0.922, respectively. This increased performance in the validation subset is attributable to the dropout layers in the model, which exclusively affect the performance in the training subset. These performance metrics generalized extremely well to the test set, in which the model outperformed its metrics in the validation set with an AUROC and AUPRC of 0.892 and 0.927, respectively. AUROC. With high AUROC and AUPRC in the held-out test set, this indicates that the model has high discrimination for distinguishing between abnormal and normal ECGs. Additionally, the high AUPRC suggests that this performance is not hindered by the imbalanced dataset, given that most ECGs in the dataset are from individuals without any ECG abnormalities. The performance in the test set exceeding the performance in the validation set indicates that the model did not overfit to the training/validation subset, and would likely generalize well to external data sources, which is imperative for any machine learning model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISCUSSION and CONCLUSIONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model successfully automates the detection of ECG abnormalities. Because the model was trained exclusively on Lead I ECGs with low sampling frequencies, is can be easily ported to the single-lead ECGs obtained on wearable and portable devices. Specifically, the algorithm demonstrates excellent discriminatory performance in a completely held-out test set with ECGs sampled as low as 100 Hz, making it ideal for wearable device-based screening strategies. Such models represent an extremely promising application for AI in advancing medical care and accelerating the speed at which cardiovascular diseases are identified and treated.  \n",
    "\n",
    "The current 12-lead ECG-based models are limited to investments by health systems to incorporate tools into digital ECG repositories, and thereby limited to individuals who already seek care in those systems. In addition to the clinically indicated ECGs limiting the scope of screening, even this technology may not be available or cost-effective for smaller hospitals and clinics with limited access to digital ECGs. Wearable devices allow obtaining ECGs that are more accessible and allow for community-wide screening, an important next step in the early detection of cardiovascular diseases. \n",
    "\n",
    "This study is limited by the size of the PTB-XL dataset. Although 21,837 ECGs are sufficient for the development of a model, an ideal model would be developed using hundreds of thousands of samples, with train, validation, and test sets being separated on the patient level instead of simply at the record level. Furthermore, a model to be deployed on wearable ECGs would necessitate validation on the specific data collection platform of that device, to ensure compatibility with the specific sensors and acquisition environments for wearable device ECGs. The model also strictly separates ECGs as abnormal, or normal. Other models, trained on more comprehensively labeled datasets, have demonstrated the ability to distinguish with greater specificity different diseases or disease combination, including diseases that are not conventionally observable using ECGs. The SCP codes used to label these ECGs are limited to ECG-diagnosable features, and do not contain diagnoses from downstream tests that may have been performed on the patients. Reducing the need for these frequently costly and resource-consuming advanced tests would be a substantial gain for the medical system.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attia, Z. I., Kapa, S., Lopez-Jimenez, F., McKie, P. M., Ladewig, D. J., Satam, G., Pellikka, P. A., Enriquez-Sarano, M., Noseworthy, P. A., Munger, T. M., Asirvatham, S. J., Scott, C. G., Carter, R. E., & Friedman, P. A. (2019). Screening for cardiac contractile dysfunction using an artificial intelligence–enabled electrocardiogram. Nature Medicine, 25(1), 70–74.\n",
    "\n",
    "Cui, Jia, Lin, & Song. (n.d.). Class-balanced loss based on effective number of samples. Proceedings of the Estonian Academy of Sciences. Biology, Ecology = Eesti Teaduste Akadeemia Toimetised. Bioloogia, Okoloogia. http://openaccess.thecvf.com/content_CVPR_2019/html/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.html\n",
    "\n",
    "Duarte, R., Stainthorpe, A., Mahon, J., Greenhalgh, J., Richardson, M., Nevitt, S., Kotas, E., Boland, A., Thom, H., Marshall, T., Hall, M., & Takwoingi, Y. (2019). Lead-I ECG for detecting atrial fibrillation in patients attending primary care with an irregular pulse using single-time point testing: A systematic review and economic evaluation. PloS One, 14(12), e0226671.\n",
    "\n",
    "Galasko, G. I. W., Barnes, S. C., Collinson, P., Lahiri, A., & Senior, R. (2006). What is the most cost-effective strategy to screen for left ventricular systolic dysfunction: natriuretic peptides, the electrocardiogram, hand-held echocardiography, traditional echocardiography, or their combination? European Heart Journal, 27(2), 193–200.\n",
    "\n",
    "Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. R. (2012). Improving neural networks by preventing co-adaptation of feature detectors. In arXiv [cs.NE]. arXiv. http://arxiv.org/abs/1207.0580\n",
    "\n",
    "Ioffe, S., & Szegedy, C. (07--09 Jul 2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In F. Bach & D. Blei (Eds.), Proceedings of the 32nd International Conference on Machine Learning (Vol. 37, pp. 448–456). PMLR.\n",
    "Kamga, P., Mostafa, R., & Zafar, S. (2022). The Use of Wearable ECG Devices in the Clinical Setting: a Review. Current Emergency and Hospital Medicine Reports, 10(3), 67–72.\n",
    "\n",
    "Khunte, A., Sangha, V., Oikonomou, E. K., Dhingra, L. S., Aminorroaya, A., Mortazavi, B. J., Coppi, A., Krumholz, H. M., & Khera, R. (2022). Detection of left ventricular systolic dysfunction from single-lead electrocardiography adapted for wearable devices. In bioRxiv. https://doi.org/10.1101/2022.12.03.22283065\n",
    "\n",
    "Seshadri, D. R., Bittel, B., Browsky, D., Houghtaling, P., Drummond, C. K., Desai, M. Y., & Gillinov, A. M. (2020). Accuracy of Apple Watch for Detection of Atrial Fibrillation. Circulation, 141(8), 702–703.\n",
    "\n",
    "Wagner, P., Strodthoff, N., Bousseljot, R.-D., Kreiseler, D., Lunze, F. I., Samek, W., & Schaeffter, T. (2020). PTB-XL, a large publicly available electrocardiography dataset. In Scientific Data (Vol. 7, Issue 1). https://doi.org/10.1038/s41597-020-0495-6 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8b01b04d493ea73eac4eb78964d49635df865828c40b121714a02c4ef05874c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
